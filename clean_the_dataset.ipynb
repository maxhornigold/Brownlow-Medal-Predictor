{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from retrieve_data import retrieve_data\n",
    "from clean_data import clean_data\n",
    "from prepare_data_for_model import get_data_for_model\n",
    "from create_the_model import create_model, train_model\n",
    "from helper import sort_by_game\n",
    "\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-a1eee19c3c8a>:2: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  data = retrieve_data()\n"
     ]
    }
   ],
   "source": [
    "# retrieve the latest data\n",
    "data = retrieve_data()\n",
    "\n",
    "# save latest dataset\n",
    "data.to_csv('../Data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"../Data/data.csv\")\n",
    "\n",
    "# clean the data\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "#cleaned_data = cleaned_data.dropna(axis=0, how='any')\n",
    "\n",
    "# save the cleaned dataset\n",
    "cleaned_data.to_csv('../Data/cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cleaned_data.columns:\n",
    "    print(\"{:>30} : {}\".format(col, cleaned_data.dtypes[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cleaned data\n",
    "cleaned_data = pd.read_csv('../Data/cleaned_dataset.csv')\n",
    "\n",
    "# obtain relevant training and testing datasets\n",
    "training_data, testing_data = get_data_for_model(cleaned_data)\n",
    "x_tr, y_tr, y_oh_tr, info_tr, match_ids_tr, player_names_tr = training_data\n",
    "x_ts, y_ts, y_oh_ts, info_ts, match_ids_ts, player_names_ts = testing_data\n",
    "\n",
    "# save the training datasets\n",
    "x_tr.to_csv('../Data/x_tr.csv', index=False)\n",
    "y_tr.to_csv('../Data/y_tr.csv', index=False)\n",
    "y_oh_tr.to_csv('../Data/y_oh_tr.csv', index=False)\n",
    "info_tr.to_csv('../Data/info_tr.csv', index=False)\n",
    "#match_ids_tr.to_csv('../Data/match_ids_tr.csv', index=False)\n",
    "#player_names_tr.to_csv('../Data/player_names_tr.csv', index=False)\n",
    "\n",
    "# save the testing datasets\n",
    "x_ts.to_csv('../Data/x_ts.csv', index=False)\n",
    "y_ts.to_csv('../Data/y_ts.csv', index=False)\n",
    "y_oh_ts.to_csv('../Data/y_oh_ts.csv', index=False)\n",
    "info_ts.to_csv('../Data/info_ts.csv', index=False)\n",
    "#match_ids_ts.to_csv('../Data/match_ids_ts.csv', index=False)\n",
    "#player_names_ts.to_csv('../Data/player_names_ts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9108, 54)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "x_ts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts_2  =x_ts.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9006, 54)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ts_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the training data\n",
    "x_tr = pd.read_csv('../Data/x_tr.csv')\n",
    "y_oh_tr = pd.read_csv('../Data/y_oh_tr.csv')\n",
    "\n",
    "# read the testing data\n",
    "x_ts = pd.read_csv('../Data/x_ts.csv')\n",
    "y_oh_ts = pd.read_csv('../Data/y_oh_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'class_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3d4756d001e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_oh_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Desktop\\All\\Personal\\Footy Figures\\Models & Products\\Brownlow Medal Model\\Code\\create_the_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, x_tr, y_oh_tr, num_epochs, batch_size, validation_split)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# train the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     history = model.fit(x_tr, y_oh_tr,\n\u001b[0m\u001b[0;32m     44\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'class_weights'"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = create_model()\n",
    "\n",
    "# train the model\n",
    "history = train_model(model, x_tr, y_oh_tr, 100, 500, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('saved_models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = models.load_model('saved_models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the model\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('Training_Model.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy of model on training & testing data\n",
    "print(model.evaluate(x_tr, y_oh_tr)[1])\n",
    "print(model.evaluate(x_ts, y_oh_ts)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities on the training set\n",
    "probs_tr = model.predict(x_tr)\n",
    "\n",
    "# sort probabilities by game\n",
    "#probs_tr_game = sort_by_game(probs_tr, info_tr)\n",
    "\n",
    "# get expected votes from predicted probabilities & scale expected votes\n",
    "#exp_votes_tr_game_raw = get_expected_votes(probs_tr_game)\n",
    "#exp_votes_tr_game = scale_expected_votes(exp_votes_tr_game_raw)\n",
    "\n",
    "# compute the game predictions based on expected votes\n",
    "#pred_votes_tr_game = compute_predictions(exp_votes_tr_game)\n",
    "\n",
    "# see confusion matrix based on predicted votes \n",
    "#see_confusion_matrix(pred_votes_tr_game, y_tr_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities on the testing set\n",
    "probs_ts = model.predict(x_ts)\n",
    "\n",
    "# sort probabilities by game\n",
    "probs_ts_game = sort_by_game(probs_ts, info_ts, game_ids_ts)\n",
    "\n",
    "# get expected votes from predicted probabilities & scale expected votes\n",
    "exp_votes_ts_game_raw = get_expected_votes(probs_ts_game)\n",
    "exp_votes_ts_game = scale_expected_votes(exp_votes_ts_game_raw)\n",
    "\n",
    "# compute the game predictions based on expected votes\n",
    "pred_votes_ts_game = compute_predictions(exp_votes_ts_game)\n",
    "\n",
    "# see confusion matrix based on predicted votes \n",
    "see_confusion_matrix(pred_votes_ts_game, y_ts_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a prediction from the testing dataset\n",
    "game_num = 25\n",
    "show_game_prediction(probs_ts_game,\n",
    "                     exp_votes_ts_game_raw,\n",
    "                     exp_votes_ts_game, \n",
    "                     pred_votes_ts_game, \n",
    "                     y_ts_game, \n",
    "                     info_ts_game, \n",
    "                     game_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort expected votes by player\n",
    "exp_votes_ts_player = sort_votes_by_player(exp_votes_ts_game, info_ts_game, player_names_ts)\n",
    "\n",
    "# sort predictions by player\n",
    "pred_votes_ts_player = sort_votes_by_player(pred_votes_ts_game, info_ts_game, player_names_ts)\n",
    "\n",
    "# compute each player's total expected votes for the 2020 season\n",
    "exp_votes_ts_player_total = compute_total_votes(exp_votes_ts_player)\n",
    "\n",
    "# compute each player's total predicted votes for the 2020 season\n",
    "pred_votes_ts_player_total = compute_total_votes(pred_votes_ts_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see best performing players based on expected votes\n",
    "show_most_votes(exp_votes_ts_player_total, player_names_ts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see best performing players based on predicted votes\n",
    "show_most_votes(pred_votes_ts_player_total, player_names_ts, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
